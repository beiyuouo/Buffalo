save_dir: "save"

data: 
  dataset: "vqa-rad"
  image_dir: "/workspace/dataset/VQA-RAD/VQA_RAD Image Folder"
  ann_path: "/workspace/dataset/VQA-RAD/data_closed.json"
  num_workers: 8
  prefetch_factor: 4
  image_size: 256
  crop_size: 224
  num_classes: 14
  niid: false
  niid_type: "feature" # "feature", "quantity"
  niid_pkl: "data/vqa-rad-niid-{}-{}.pkl"
  niid_alpha: 0.5

task:
  type: "vqa"
  prompt: ""
  iuxray_template: "Human: <Img><ImageHere></Img> {} \nAssistant:"

model:
  cache_dir: "cache"
  visual_encoder: 
    model_name: "vit" # "resnet101"
    pretrained: false
    # pretrained_path: "cache/vit/ViT-B_16.npz"
    hf_backbone: "microsoft/swin-base-patch4-window7-224"
    global_only: false
    freeze: false
    use_lora: true
    lora:
      r: 16
      alpha: 16
      dropout: 0.1
  tokenizer:
    eos_token: "</s>"
  text_decoder:
    model_name: "llama"
    # hf_backbone: "cache/LLaMA3/llama-3-8b-instruct/" # "emilyalsentzer/Bio_ClinicalBERT"
    hf_backbone: "/workspace/hf/LLaMA2/llama-2-7b-chat/" # "emilyalsentzer/Bio_ClinicalBERT"
    can_batch_generate: true
    load_in_8bit: true
    load_in_4bit: false
    use_lora: false
    lora:
      r: 16
      alpha: 16
      dropout: 0.1
  
  delta_file: null
  max_q_length: 20
  max_length: 3
  min_new_tokens: 1
  max_new_tokens: 5
  length_penalty: 2.0
  temperature: null
  top_p: null
  num_beams: 3
  top_p: 0.9
  repetition_penalty: 2.0
  do_sample: false


seed: 3094
n_nodes: 1
n_gpus: 1
gpu_ids: "0"
accelerator: "gpu"
strategy: "ddp"
precision: "bf16-mixed"
version: "1"

train:
  algor: "fedavg"
  modal_hetero: true
  mm_strategy: "prototype" # "drop", "duplicate", "mean", "prototype"
  mm_mean_type: "token" # "token", "original"
  cross_fedproto:
    k: 20
  num_types: 3 # 0 for full, 1 for frontal, 2 for lateral, 3 for text
  num_rounds: 20
  num_clients: 3
  num_selected: 2 # 2 for debugging, 7 for full
  num_epochs: 3 # 1 for debugging, 3 for full
  batch_size: 4
  val_batch_size: 4
  test_batch_size: 4
  
  lr: 0.001
  lr_ve: 0.001
  lr_proj: 0.0003
  lr_ed: 0.001

  monitor_metrics: ['Bleu_4', 'CIDEr']
  monitor_weights: [0.5, 0.5]

  val_check_interval: null
  limit_val_batches: 1.0
  num_sanity_val_steps: 0 # 2 for debugging
  accumulate_grad_batches: 1
  check_val_every_n_epoch: 3
  every_n_train_steps: 0
  every_n_epochs: 0

  loss:
    weight_proto: 0.001
    weight_sim: 0.001